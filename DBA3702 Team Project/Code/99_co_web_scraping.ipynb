{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Data Curation (Web Scraping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary dependencies\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import ElementClickInterceptedException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set wait times\n",
    "waittime = 5\n",
    "sleeptime = 1\n",
    "\n",
    "# Initiate web driver\n",
    "try:\n",
    "    driver.close() # Close any existing WebDrivers\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Define target website\n",
    "home_page = \"https://www.99.co/singapore/rent\"\n",
    "\n",
    "# Set webdriver options\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('ignore-certificate-errors')\n",
    "\n",
    "# Initiate webdriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options = options)\n",
    "# Get driver to retrieve homepage\n",
    "driver.get(home_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get last page number (of all listings)\n",
    "def get_last_page_number():\n",
    "    pagination_elems = driver.find_elements(By.CLASS_NAME, \"SearchPagination\") \n",
    "    page_numbers = [elem.text.split(\"\\n\") for elem in pagination_elems][0]\n",
    "    last_page = int(page_numbers[-2])\n",
    "    return last_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate all web links on current listings page\n",
    "def collate_web_links():\n",
    "    elems = driver.find_elements(By.CSS_SELECTOR, \"._31ajL [href]\") # Div class just above div style=\"opacity:1\"\n",
    "    links = [elem.get_attribute('href') for elem in elems] # Get the web links present on current page\n",
    "    \n",
    "    substring = '/singapore/rent/property/'\n",
    "    # Truncate web links to remove unnecessary last part of string\n",
    "    condo_links = [link.split(\"?\", 1)[0] for link in links if substring in link] # List comprehension ensuring link is directed to property page, not ad\n",
    "    \n",
    "    return condo_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape detailed page (New Web Template)\n",
    "def scrape_page_new():\n",
    "        \n",
    "    details_dict = {}\n",
    "\n",
    "    # Scrape info on page\n",
    "    district_class_name = 'dniCg _3j72o _2rhE-'\n",
    "    rental_price_class_name = \"_1zGm8 _3na6W _1vzK2\"\n",
    "    class_name_1 = \"_2sIc2 _29qfj _2rhE-\"\n",
    "    title_class_name = \"_3Wogd JMF8h lFqTi _1vzK2\"\n",
    "    WebDriverWait(driver, waittime).until(EC.presence_of_element_located((By.XPATH, f\"//h1[@class='{title_class_name}']\")))\n",
    "    details_dict['title'] = driver.find_element(By.XPATH,f\"//h1[@class='{title_class_name}']\").text #Updated\n",
    "    details_dict['rental'] = driver.find_element(By.XPATH,f\"//p[@class='{rental_price_class_name}']\").text #Updated\n",
    "    row = driver.find_elements(By.XPATH,f\"//p[@class='{class_name_1}']\")\n",
    "    if (len(row) == 2): \n",
    "        details_dict['bed'] = row[0].text #Updated\n",
    "        details_dict['sqft'] = row[1].text #Updated\n",
    "    else:\n",
    "        details_dict['bed'] = row[0].text #Updated\n",
    "        details_dict['bath'] = row[1].text #Updated\n",
    "        details_dict['sqft'] = row[2].text #Updated\n",
    "        if (len(row) == 4):\n",
    "            details_dict['sqft'] = details_dict['sqft'] + \" / \" + row[3].text #Updated\n",
    "\n",
    "    address_details = driver.find_elements(By.XPATH,f\"//p[@class='{district_class_name}']//span\")\n",
    "    if (len(address_details) > 1):\n",
    "        details_dict['address'] = address_details[0].text #Updated\n",
    "        details_dict['district'] = address_details[len(address_details)-1].text #Updated\n",
    "    elif (len(address_details) == 1):\n",
    "        details_dict['district'] = address_details[0].text #Updated\n",
    "    # This section will take into account these details (if present): Availability, Lease, Furnishing, Property Type, \n",
    "    # Name, Unit Types, Total Units, Built Year, Tenure, Developer, and Neighbourhood\n",
    "    class_name_td1 = '_3r4yN NomDX'\n",
    "    class_name_td2 = '_3r4yN XCAFU'\n",
    "    \n",
    "    map_element = driver.find_element(By.ID,\"propertyDetails\")\n",
    "    actions = ActionChains(driver)\n",
    "    actions.move_to_element(map_element).perform()\n",
    "    \n",
    "    WebDriverWait(driver, waittime).until(EC.presence_of_element_located((By.XPATH, f\"//table[@class='_3NpKo']\")))\n",
    "    num_of_property_details = len(driver.find_elements(By.XPATH,f\"//td[contains(@class, '{class_name_td1}')]\"))\n",
    "    time.sleep(sleeptime)\n",
    "    for i in range(num_of_property_details):\n",
    "        detail_category = driver.find_elements(By.XPATH,f\"//td[contains(@class, '{class_name_td1}')]\")[i].text\n",
    "        detail_category = detail_category.lower()\n",
    "        detail_category = detail_category.replace(\" \",\"_\")\n",
    "        details_dict[f'{detail_category}'] = driver.find_elements(By.XPATH,f\"//td[contains(@class, '{class_name_td2}')]\")[i].text\n",
    "\n",
    "    # MRT\n",
    "    try:\n",
    "        details_dict['nearest_mrt_name'] = driver.find_element(By.XPATH,f\"//p[@class='_2sIc2 _2rhE- _1c-pJ']//a\").text\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "#     # Expand amenities\n",
    "#     button_class = 'cFGt2 _1P_YF' # For amenities\n",
    "#     WebDriverWait(driver, waittime).until(EC.presence_of_element_located((By.XPATH, f\"//button[@class='{button_class}']\")))\n",
    "#     driver.find_element(By.XPATH,f\"//button[@class='{button_class}']\").click()\n",
    "\n",
    "#     # Extract all amenities \n",
    "#     amenities_elems = driver.find_elements(By.XPATH,f\"//p[@class='_2sIc2 AIgs2 _2rhE-']\")\n",
    "#     details_dict['amenities'] = [str(elem.text) for elem in amenities_elems]\n",
    "\n",
    "#     details_dict['electoral_div'] = driver.find_element(By.XPATH,f\"//h2[@class='Z0npN _3NW6g']\")[0].text\n",
    "\n",
    "    # Scroll down to reveal Google map section\n",
    "    map_element = driver.find_element(By.ID,\"location\")\n",
    "    actions = ActionChains(driver)\n",
    "    actions.move_to_element(map_element).perform()\n",
    "    \n",
    "    class_name_location_div = '_3OnRG'\n",
    "    class_name_location = 'yMCxv _1YwzE _1vzK2'\n",
    "    try:\n",
    "        details_dict['travel_time_changi'] = WebDriverWait(driver, waittime).until(EC.presence_of_element_located((By.XPATH, f\"//div[@class='{class_name_location_div}']/div[@class='{class_name_location}']\"))).text\n",
    "        details_dict['travel_time_raffles'] = driver.find_elements(By.XPATH,f\"//div[@class='{class_name_location_div}']/div[@class='{class_name_location}']\")[1].text\n",
    "        details_dict['travel_time_orchard'] = driver.find_elements(By.XPATH,f\"//div[@class='{class_name_location_div}']/div[@class='{class_name_location}']\")[2].text\n",
    "    except TimeoutException:\n",
    "        pass\n",
    "    return details_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open web link and scrape information on that page\n",
    "def open_and_scrape(web_link, master_list_of_dict):\n",
    "        \n",
    "    # Switch to web link\n",
    "    driver.get(web_link)\n",
    "    \n",
    "    # Scrape page based on whether old web design or new design\n",
    "    # Old version has the class name \"_2yeD-\" just below the <div id = appContent>\n",
    "    details_dict = scrape_page_new()\n",
    "    master_list_of_dict.append(details_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the dictionaries (each listing should give one dictionary)\n",
    "master_list_of_dict = []\n",
    "\n",
    "# Get the last page number in terms of pages of available listings\n",
    "last_page_num = get_last_page_number()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_scraping(start_page, end_page):\n",
    "    for i in range(1, end_page+1):\n",
    "        try:\n",
    "            current_page = WebDriverWait(driver, waittime).until(EC.presence_of_element_located((By.XPATH, \"//li[@class='active']\"))).text\n",
    "            current_page = int(current_page)\n",
    "        except:\n",
    "            print(f\"Skip\")\n",
    "            pass\n",
    "\n",
    "        if current_page < end_page:\n",
    "            condo_links = collate_web_links()\n",
    "            for index,link in enumerate(condo_links):\n",
    "                open_and_scrape(link, master_list_of_dict)\n",
    "                print(f\"Completed {index+1} out of {len(condo_links)} links of Page {start_page}\")\n",
    "            start_page += 1\n",
    "            try:\n",
    "                next_link = f'{home_page}?page_num={start_page}'\n",
    "                driver.get(next_link)\n",
    "            except:\n",
    "                time.sleep(10)\n",
    "                next_link = f'{home_page}?page_num={start_page}'\n",
    "                driver.get(next_link)\n",
    "\n",
    "        else:\n",
    "            try:\n",
    "                condo_links = collate_web_links()\n",
    "                for link in condo_links:\n",
    "                    open_and_scrape(link, master_list_of_dict)\n",
    "                print('Scraping complete')\n",
    "                break\n",
    "            except:\n",
    "                print(f\"Skip {current_page}\")\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "execute_scraping(1, last_page_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4510"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of dictionaries in the list\n",
    "len(master_list_of_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to remove duplicate entries in the list of dictionaries\n",
    "def remove_dupe_dicts(l):\n",
    "    list_of_strings = [\n",
    "    json.dumps(d, sort_keys=True)\n",
    "    for d in l]\n",
    "\n",
    "    list_of_strings = set(list_of_strings)\n",
    "\n",
    "    return [json.loads(s) for s in list_of_strings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_list = remove_dupe_dicts(master_list_of_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of listings successfully scraped\n",
    "len(master_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as JSON file\n",
    "with open('master_list_cleaned_v3.json', 'w') as file:\n",
    "    file.write(json.dumps(master_list, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json (r'master_list_cleaned_v3.json')\n",
    "df.to_csv (r'99_co.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
